{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820d02c-d58d-46e9-bd58-5e77ee3fdd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa7a1c-f506-47d4-8c0f-321a9857b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подаётся:\n",
    "\n",
    "filters = {\n",
    "    'isMSP': request.query_params.get('isMSP'),\n",
    "    'companySize': request.query_params.get(‘companySize'),\n",
    "    'cityActual': request.query_params.get('cityActual'),\n",
    "    'shipper’: request.query_params.get('shipper'),\n",
    "    'consignee': request.query_params.get('consignee')\n",
    "}\n",
    "\n",
    "1) isMSP - Находится в реестре МСП\n",
    "2) companySize - Размер компании.Наименование\n",
    "3) cityActual - Город фактический\n",
    "4) shipper - Грузоотправитель\n",
    "5) consignee - Грузополучатель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751ce80-9a11-489c-ba46-050a7f854434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нужно для предсказания:\n",
    "\n",
    "'ID','Размер уставного капитала объявленный', 'Численность персонала по данным ФНС.Количество', \n",
    "    'Карточка клиента (внешний источник).Индекс платежной дисциплины Значение', \n",
    "    'Карточка клиента (внешний источник).Индекс финансового риска Значение',\n",
    "    'Провозная плата (период 1)', 'Провозная плата (период 2)',\n",
    "    'Объем перевозок(тн) (период 1)', 'Объем перевозок(тн) (период 2)',\n",
    "    'positive_action'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac7b4c-904b-4108-9560-97826f66af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нужно вывести:\n",
    "[\n",
    "    {'id': 11, 'leavingChance': 0.85},\n",
    "    {'id': 14, 'leavingChance': 0.25},\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1818026-24f9-49c4-8d78-78048e5cf49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# СОдержимое папки uploaded_files:\n",
    "\n",
    "r\"uploaded_files\\Выгрузка_маркетинговые списки\\МС_*.xls\"\n",
    "r\"uploaded_files\\Выгрузки_интересы+обращения+объёмы перевозок\\Объёмы перевозок.xls\"\n",
    "r\"uploaded_files\\Выгрузки_интересы+обращения+объёмы перевозок\\Обращения.xls\"\n",
    "r\"uploaded_files\\Выгрузки_интересы+обращения+объёмы перевозок\\Интересы.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9701e77-d971-4c16-acf2-8f998b8ad946",
   "metadata": {},
   "source": [
    "# Подготовка данных:\n",
    "1) 'Провозная плата (период 1)'\n",
    "2) 'Провозная плата (период 2)'\n",
    "3) 'Объем перевозок(тн) (период 1)'\n",
    "4) 'Объем перевозок(тн) (период 2)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9506b6-d718-4047-8f5a-f3ea05837432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64b88140-b2b9-492e-b20d-41fca6e63df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл найден: uploaded_files\\Выгрузки_интересы+обращения+объёмы перевозок\\Объёмы перевозок.xls\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_file(filename, search_path):\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "file_to_find = 'Объёмы перевозок.xls'\n",
    "search_directory = 'uploaded_files'  # Changed to search in 'uploaded_files'\n",
    "\n",
    "file_path = find_file(file_to_find, search_directory)\n",
    "\n",
    "if file_path:\n",
    "    print(f\"Файл найден: {file_path}\")\n",
    "    data = pd.read_excel(file_path, header=2)  # Adjust header if necessary\n",
    "else:\n",
    "    print(\"Файл не найден.\")\n",
    "    \n",
    "data = pd.read_excel(file_path, header=2)\n",
    "transportation_volumes = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "948f16d9-308b-4ced-a79e-b1ce64b27545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фильтрация завершена. Результат сохранен в: uploaded_files\\Объёмы_перевозок_filtered.xlsx\n",
      "Используются месяцы:  ['2024/08', '2024/07', '2024/06', '2024/05', '2024/04', '2024/03', '2024/02', '2024/01', '2023/12', '2023/11', '2023/10', '2023/09']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = transportation_volumes\n",
    "output_file = r'uploaded_files\\Объёмы_перевозок_filtered.xlsx'\n",
    "\n",
    "data = pd.read_excel(input_file, header=1)\n",
    "\n",
    "# Убираем лишние пробелы в названиях колонок и переименовываем\n",
    "data.columns = data.columns.str.strip()\n",
    "data.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)\n",
    "\n",
    "# Определяем текущую дату\n",
    "current_year = 2024\n",
    "current_month = 10  \n",
    "\n",
    "# Последние 12 доступных месяцев\n",
    "available_months = []\n",
    "\n",
    "# Находим доступные месяцы\n",
    "for year in range(current_year, 2019, -1):\n",
    "    for month in range(12, 0, -1):\n",
    "        month_str = f'{year}/{month:02d}'\n",
    "        if month_str in data.columns and f'{month_str}.1' in data.columns:\n",
    "            available_months.append(month_str)\n",
    "            if len(available_months) == 12:\n",
    "                break\n",
    "    if len(available_months) == 12:\n",
    "        break\n",
    "\n",
    "if len(available_months) < 12:\n",
    "    print(\"Недостаточно месяцев для анализа. Найденные месяцы:\", available_months)\n",
    "else:\n",
    "    period_1 = available_months[:6]\n",
    "    period_2 = available_months[6:12]\n",
    "\n",
    "    # Выборка данных для периодов\n",
    "    провозная_плата_1 = data[period_1].sum(axis=1)\n",
    "    объем_перевозок_1 = data[[f'{month}.1' for month in period_1]].sum(axis=1)\n",
    "\n",
    "    провозная_плата_2 = data[period_2].sum(axis=1)\n",
    "    объем_перевозок_2 = data[[f'{month}.1' for month in period_2]].sum(axis=1)\n",
    "\n",
    "    results = {\n",
    "        'ID': data['ID'],\n",
    "        'Провозная плата (период 1)': провозная_плата_1.where(провозная_плата_1.notna(), None),\n",
    "        'Объем перевозок(тн) (период 1)': объем_перевозок_1.where(провозная_плата_1.notna(), None),\n",
    "        'Провозная плата (период 2)': провозная_плата_2.where(провозная_плата_2.notna(), None),\n",
    "        'Объем перевозок(тн) (период 2)': объем_перевозок_2.where(провозная_плата_2.notna(), None),\n",
    "    }\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Убираем строки с отсутствующими данными\n",
    "    result_df.dropna(subset=['Провозная плата (период 1)', 'Объем перевозок(тн) (период 1)', \n",
    "                             'Провозная плата (период 2)', 'Объем перевозок(тн) (период 2)'], inplace=True)\n",
    "\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    print(\"Фильтрация завершена. Результат сохранен в:\", output_file)\n",
    "\n",
    "print(\"Используются месяцы: \", available_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac3c68d0-bf2f-4e94-8d68-56a8696fcf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# убирает непотребства \n",
    "file_path = r'uploaded_files\\Объёмы_перевозок_filtered.xlsx'\n",
    "df = pd.read_excel(file_path, header=1)  # header=1 пропускает первую строку\n",
    "\n",
    "df.columns = [\n",
    "    'ID', \n",
    "    'Провозная плата (период 2)', \n",
    "    'Объем перевозок(тн) (период 2)', \n",
    "    'Провозная плата (период 1)', \n",
    "    'Объем перевозок(тн) (период 1)'\n",
    "]\n",
    "\n",
    "df_summary = df.groupby('ID').sum().reset_index()\n",
    "output_path = r'uploaded_files\\Объёмы_перевозок_filtered.xlsx'\n",
    "df_summary.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c714e38-9038-41b5-969a-58d2a4f7f67a",
   "metadata": {},
   "source": [
    "# Подготовка данных + объединение с таблицей Объёмы_перевозок_filtered.xlsx\n",
    "\n",
    "1) 'ID'\n",
    "2) 'Размер уставного капитала объявленный'\n",
    "3)  'Численность персонала по данным ФНС.Количество'\n",
    "4)  'Карточка клиента (внешний источник).Индекс платежной дисциплины Значение'\n",
    "5)  'Карточка клиента (внешний источник).Индекс финансового риска Значение'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ad7f1a4-de5b-4b7c-af21-7d39c2dc5633",
   "metadata": {},
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64a57383-83ed-4d36-b558-73a8e6a6a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "\n",
    "# Читаем данные из файла с использованием pandas\n",
    "file_path = r'uploaded_files\\Объёмы_перевозок_filtered.xlsx'\n",
    "df_original = pd.read_excel(file_path)\n",
    "\n",
    "# Преобразуем в Dask DataFrame\n",
    "df_original = dd.from_pandas(df_original, npartitions=1)\n",
    "\n",
    "# Получаем все уникальные значения ID как массив\n",
    "unique_ids = df_original['ID'].dropna().unique().compute()\n",
    "\n",
    "# Создаем список для DataFrames всех файлов\n",
    "dataframes = []\n",
    "\n",
    "# Проходим по каждому файлу в указанной директории\n",
    "for file in glob.glob(r\"uploaded_files\\Выгрузка_маркетинговые списки\\МС_*.xls\"):\n",
    "    df = pd.read_excel(file)\n",
    "    # Преобразуем в Dask DataFrame\n",
    "    df = dd.from_pandas(df, npartitions=1)\n",
    "\n",
    "    # Фильтруем строки по уникальным ID\n",
    "    filtered_df = df[df['ID'].isin(unique_ids)]\n",
    "    dataframes.append(filtered_df)  # Добавляем отфильтрованные данные в список\n",
    "\n",
    "# Объединяем все DataFrames в один\n",
    "if dataframes:  # Только если список не пуст\n",
    "    combined_df = dd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    combined_df = dd.from_pandas(pd.DataFrame(), npartitions=1)  # Если нет файлов, создаем пустой Dask DataFrame\n",
    "\n",
    "# Объединяем с оригинальным DataFrame по ID\n",
    "final_df = df_original.merge(combined_df, on='ID', how='left')\n",
    "\n",
    "# Преобразуем финальный DataFrame в pandas перед сохранением\n",
    "final_df = final_df.compute()\n",
    "\n",
    "# Сохраняем результат в новый Excel файл\n",
    "output_path = r'uploaded_files\\test.xlsx'\n",
    "final_df.to_excel(output_path, index=False)  # Теперь save работает без ошибок\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eb0a52e-f89c-4276-98dd-1c7fffc984e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'uploaded_files\\test.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "selected_columns = [\n",
    "    'ID',\n",
    "    'Находится в реестре МСП',\n",
    "    'Размер компании.Наименование',\n",
    "    'Город фактический',\n",
    "    'Грузоотправитель',\n",
    "    'Грузополучатель',\n",
    "    'Размер уставного капитала объявленный',\n",
    "    'Численность персонала по данным ФНС.Количество',\n",
    "    'Карточка клиента (внешний источник).Индекс платежной дисциплины Значение',\n",
    "    'Карточка клиента (внешний источник).Индекс финансового риска Значение',\n",
    "    'Провозная плата (период 1)',\n",
    "    'Провозная плата (период 2)',\n",
    "    'Объем перевозок(тн) (период 1)',\n",
    "    'Объем перевозок(тн) (период 2)'\n",
    "]\n",
    "df_filtered = df[selected_columns]\n",
    "\n",
    "\n",
    "df_filtered = df_filtered.drop_duplicates(subset='ID', keep='last')\n",
    "\n",
    "\n",
    "output_path = r'uploaded_files\\test.xlsx'\n",
    "df_filtered.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e398d03-5ba5-451b-a1bc-5791e31e5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление строк по Да, Нет\n",
    "file_path = r'uploaded_files\\test.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df_cleaned = df[df['Находится в реестре МСП'].isin(['Да', 'Нет'])]\n",
    "\n",
    "df_cleaned.to_excel(r'uploaded_files\\test.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12712f24-df25-4dd2-bc69-b489c47dfce0",
   "metadata": {},
   "source": [
    "## Подготовка данных:\n",
    "### positive_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea149288-94ed-42c8-a7ee-b08ba6d26f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_file_path = r'uploaded_files\\Выгрузки_интересы+обращения+объёмы перевозок\\Обращения.xls'\n",
    "activity_df = pd.read_excel(activity_file_path)\n",
    "\n",
    "\n",
    "filtered_data_path = r'uploaded_files\\test.xlsx'\n",
    "filtered_df = pd.read_excel(filtered_data_path)\n",
    "\n",
    "topics_of_interest = [\n",
    "    \"Цифровые сервисы\",\n",
    "    \"Электронный обмен документами\",\n",
    "    \"Перевозка грузов и порожних вагонов\",\n",
    "    \"Оформление документов связанных с перевозкой грузов\",\n",
    "    \"Заключение договоров\",\n",
    "    \"Финансовые расчеты\",\n",
    "    \"Перевозка грузов в контейнерах\",\n",
    "    \"Благодарности\",\n",
    "    \"Перевозка грузов в рефрижераторных секциях ИВ-термосах вагонах-термосах\",\n",
    "    \"Перевозка сборной/мелкой партии грузов / разовая\"\n",
    "]\n",
    "\n",
    "\n",
    "filtered_activity_df = activity_df[activity_df['Тема вопроса'].isin(topics_of_interest)]\n",
    "positive_action_counts = filtered_activity_df['ID'].value_counts()\n",
    "\n",
    "filtered_df['positive_action'] = filtered_df['ID'].map(positive_action_counts).fillna(0)\n",
    "\n",
    "\n",
    "output_path = r'uploaded_files\\test.xlsx'\n",
    "filtered_df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224aef4-00d1-4d1e-b8ff-ff59d448c1b6",
   "metadata": {},
   "source": [
    "## Предсказание для каждого ID\n",
    "### Загрузка Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72bf114c-7112-4ddf-9d85-ca9c91e74637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4ae5bd5-ec53-449b-8a7e-2c539e49e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_excel(r'uploaded_files\\test.xlsx')\n",
    "df = pd.DataFrame(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e4ddef-e39e-45b1-bf1f-63ed595acc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Загрузка модели\n",
    "with open(r'Models\\random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_rf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e04c99df-6134-42ff-8708-3ff5f1a4c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[['Размер уставного капитала объявленный', 'Численность персонала по данным ФНС.Количество', \n",
    "    'Карточка клиента (внешний источник).Индекс платежной дисциплины Значение', \n",
    "    'Карточка клиента (внешний источник).Индекс финансового риска Значение',\n",
    "    'Провозная плата (период 1)', 'Провозная плата (период 2)',\n",
    "    'Объем перевозок(тн) (период 1)', 'Объем перевозок(тн) (период 2)',\n",
    "    'positive_action'\n",
    "    ]] = np.log1p(df[['Размер уставного капитала объявленный', 'Численность персонала по данным ФНС.Количество', \n",
    "    'Карточка клиента (внешний источник).Индекс платежной дисциплины Значение', \n",
    "    'Карточка клиента (внешний источник).Индекс финансового риска Значение',\n",
    "    'Провозная плата (период 1)', 'Провозная плата (период 2)',\n",
    "    'Объем перевозок(тн) (период 1)', 'Объем перевозок(тн) (период 2)',\n",
    "    'positive_action']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5da57bc-95a6-40a6-884c-f2d5b2a7d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['ID','Размер уставного капитала объявленный', 'Численность персонала по данным ФНС.Количество', \n",
    "    'Карточка клиента (внешний источник).Индекс платежной дисциплины Значение', \n",
    "    'Карточка клиента (внешний источник).Индекс финансового риска Значение',\n",
    "    'Провозная плата (период 1)', 'Провозная плата (период 2)',\n",
    "    'Объем перевозок(тн) (период 1)', 'Объем перевозок(тн) (период 2)',\n",
    "    'positive_action']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d905614-67e7-4327-8469-4d2fcb497c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Предсказания\n",
    "rf_predictions = loaded_rf_model.predict(df.drop(columns='ID'))\n",
    "\n",
    "# Обратно трансформируем предсказания\n",
    "rf_predictions_normal = np.expm1(rf_predictions)\n",
    "\n",
    "# Создаём DataFrame с предсказаниями\n",
    "results = pd.DataFrame({\n",
    "    'ID': df['ID'],\n",
    "    'Predicted_Devotion_RF': rf_predictions_normal\n",
    "})\n",
    "\n",
    "#results.head().style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e36ce4af-facf-4f52-9215-1c271b5386ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ваши предсказания\n",
    "results = pd.DataFrame({\n",
    "    'ID': df['ID'],\n",
    "    'Predicted_Devotion_RF': rf_predictions_normal\n",
    "})\n",
    "\n",
    "# Загружаем данные из Excel\n",
    "test_data = pd.read_excel(r'uploaded_files\\test.xlsx')\n",
    "\n",
    "# Объединяем данные по ID, если нужно, а если просто добавляем столбец leavingChance, то:\n",
    "results['leavingChance'] = 100 - results['Predicted_Devotion_RF']\n",
    "\n",
    "#print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ead12cf-7f22-42c0-92e1-82118f9530b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'leavingChance_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muploaded_files\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleavingChance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m-\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Devotion_RF\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleavingChance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m test_data\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muploaded_files\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Hackathon_PVD\\env\\lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Hackathon_PVD\\env\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Hackathon_PVD\\env\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Hackathon_PVD\\env\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    837\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    849\u001b[0m         join_index,\n\u001b[0;32m    850\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    856\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Hackathon_PVD\\env\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2757\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2755\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2760\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'leavingChance_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'ID': df['ID'],\n",
    "    'Predicted_Devotion_RF': rf_predictions_normal\n",
    "})\n",
    "\n",
    "test_data = pd.read_excel(r'uploaded_files\\test.xlsx')\n",
    "results['leavingChance'] = 100 - results['Predicted_Devotion_RF']\n",
    "test_data = test_data.merge(results[['ID', 'leavingChance']], on='ID', how='left')\n",
    "\n",
    "test_data.to_excel(r'uploaded_files\\test.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8619f-4c71-402f-af0d-084a3bd4b848",
   "metadata": {},
   "source": [
    "# Фильтрация по входным значениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f006767b-6d70-4368-bdf2-82ddafc0e733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': 8733, 'leavingChance': 50.17265193061121}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_file(filename, search_path):\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "def filter_data(query_params):\n",
    "    filename = 'test.xlsx'\n",
    "    search_path = '.'  # Путь к директории, в которой начнем поиск. '.' - это текущая директория.\n",
    "\n",
    "    # Поиск файла\n",
    "    file_path = find_file(filename, search_path)\n",
    "    \n",
    "    if file_path is None:\n",
    "        raise FileNotFoundError(f\"Файл '{filename}' не найден в '{search_path}'.\")\n",
    "\n",
    "    # Загрузка данных из Excel файла\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # Извлечение параметров фильтрации\n",
    "    filters = {\n",
    "        'isMSP': query_params.get('isMSP'),\n",
    "        'companySize': query_params.get('companySize'),\n",
    "        'cityActual': query_params.get('cityActual'),\n",
    "        'shipper': query_params.get('shipper'),\n",
    "        'consignee': query_params.get('consignee')\n",
    "    }\n",
    "\n",
    "    # Инициализация отфильтрованных данных\n",
    "    filtered_data = data\n",
    "\n",
    "    # Применение фильтров только если они не пустые или NaN\n",
    "    if pd.notna(filters['isMSP']) and filters['isMSP'] != '':\n",
    "        filtered_data = filtered_data[filtered_data['Находится в реестре МСП'] == filters['isMSP']]\n",
    "    if pd.notna(filters['companySize']) and filters['companySize'] != '':\n",
    "        filtered_data = filtered_data[filtered_data['Размер компании.Наименование'] == filters['companySize']]\n",
    "    if pd.notna(filters['cityActual']) and filters['cityActual'] != '':\n",
    "        filtered_data = filtered_data[filtered_data['Город фактический'] == filters['cityActual']]\n",
    "    if pd.notna(filters['shipper']) and filters['shipper'] != '':\n",
    "        filtered_data = filtered_data[filtered_data['Грузоотправитель'] == filters['shipper']]\n",
    "    if pd.notna(filters['consignee']) and filters['consignee'] != '':\n",
    "        filtered_data = filtered_data[filtered_data['Грузополучатель'] == filters['consignee']]\n",
    "\n",
    "    # Ограничение на вывод 50 записей\n",
    "    result = filtered_data[['ID', 'leavingChance']].head(50).to_dict(orient='records')\n",
    "\n",
    "    return result\n",
    "\n",
    "# Пример использования:\n",
    "query_params = {\n",
    "    'isMSP': 'Да',\n",
    "    'companySize': 'Крупный бизнес',\n",
    "    'cityActual': '',\n",
    "    'shipper': 'Да',\n",
    "    'consignee': 'Да'\n",
    "}\n",
    "\n",
    "try:\n",
    "    output = filter_data(query_params)\n",
    "    print(output)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542527d-73bb-41ea-82e3-abcf7437ee4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88914300-7f5b-4a43-bed5-f9c17bba30c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7d9f5-984a-46f4-a125-64f574de1b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285c5bf-bf93-4134-b1b5-e5f7f9371e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de87bc9-3e7d-4b0a-bab8-38c8d94adbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361aee06-a98b-44ab-b252-e05ae9054164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3574a70-74c2-4f33-8134-b41630b8315b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
